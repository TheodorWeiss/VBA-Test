Понял. Ты хочешь автоматически нормализовать “обрубки” (типа adventsk, adventskal, adventskerz…) так, чтобы в итоге они превращались в “полные” токены без ручного словаря — и при этом учитывать, что один обрубок может означать разные вещи (kalender/kerzen).

Сразу честно: на 100% без ошибок это не бывает, но можно сделать очень рабочий автомат в 2 слоя:
	1.	генерация кандидатов (“во что мог быть нормализован этот обрубок”)
	2.	выбор кандидата по контексту (по соседним токенам в этом же артикуле)

Ниже — реалистичный способ в Power Query, который масштабируется.

⸻

0) Что должно быть на входе

У тебя уже есть TokenTable:

(Artikelnummer, Token) — один токен на строку.

Это идеально.

⸻

1) Найти “семейства” токенов автоматически (без словаря)

Идея: токены, которые начинаются одинаково, образуют семейство. Внутри семейства есть длинные “нормальные” формы и короткие “обрубки”.

В Power Query
	1.	В TokenTable сделай TokenStats (ты уже делал):

	•	Token
	•	TF (count)

	2.	Добавь колонки:

	•	Len = Text.Length([Token])
	•	Prefix6 = Text.Start([Token], 6)  (6 обычно лучше 4–5, меньше шум)

	3.	Сделай таблицу Families:

	•	Group by Prefix6
	•	DistinctTokens = Anzahl unterschiedlicher Token
	•	MaxLen = Maximum Len
	•	SumTF = Sum TF

Фильтр “семейства интересны”, например:
	•	DistinctTokens >= 4
	•	MaxLen >= 9

Это автоматически поймает advents*, licht*, schoko*, и т.д.

⸻

2) Для каждого обрубка собрать кандидатов “возможные полные формы”

Определим “обрубок”:
	•	Len <= 8 (настройка)
	•	и есть хотя бы один более длинный токен с тем же Prefix6

PQ-логика

Создай таблицу ShortTokens из TokenStats:
	•	Len <= 8
	•	join на Families, где MaxLen > Len

Создай таблицу LongTokens:
	•	Len >= 9
	•	тоже с Prefix6

Теперь сделай join:
ShortTokens → LongTokens по Prefix6

Получишь кандидатов:
(ShortToken, CandidateLongToken)

Но кандидатов может быть много — нужно сузить.

⸻

3) Контекст как “решающий фактор” (автоматически)

Ключевая идея: обрубок берёт смысл из окружения.

3.1 Построить “контекстный профиль” для каждого токена

Собираем co-occurrence: какие токены рядом (в том же артикуле) встречаются с данным токеном.

Как:
	1.	Self-join TokenTable на Artikelnummer
	2.	получишь пары (Token, ContextToken) для каждого артикула
	3.	отфильтровать Token <> ContextToken
	4.	Group by (Token, ContextToken) → CoCount

Это тяжёленько, но на 20k строк обычно ок, если PQ не совсем дохлый. Если токенов много — ограничиваемся только нужными семействами (см. ниже).

3.2 Сузить объём: считать контекст только для “семейств”

Практично:
	•	ограничь Token только теми, кто входит в ShortTokens и LongTokens (из интересных семейств)
	•	тогда self-join будет не по всем 12k токенам

⸻

4) Выбор нормализации по контексту (снимает двусмысленность)

Теперь самое важное: как решить adventsk → kalender или kerzen?

Принцип

Для каждого кандидата LongToken строим “топ-контекст” (например, Top 30 ContextToken по CoCount).
Для конкретного артикула берём его токены (кроме short token) как “контекст строки”.
И выбираем кандидата, у которого совпадение по контексту максимальное.

Простая и очень рабочая метрика

Score(article, candidate) = сумма CoCount(candidate, contextToken) по всем токенам строки.

То есть:
	•	если в строке есть kerzen/teelicht/wachs, то кандидат adventskerzen… набирает больше
	•	если есть kalender/fenster/schoko, то победит adventskalender…

⸻

5) Как это собрать в Power Query без боли (пошагово)

Я опишу именно процесс, который у тебя получится сделать “мышкой” + 1–2 custom columns.

Шаг A — Таблица ArticleContext

Из TokenTable:
	•	Group by Artikelnummer → AllTokens (Alle Zeilen)
	•	Добавь колонку ContextList = [AllTokens][Token] (список токенов)

(Это список токенов строки.)

Шаг B — Таблица CandidateMap

То, что мы сделали в пункте 2:
	•	ShortToken, CandidateLongToken

Шаг C — Таблица CandidateCo

Из co-occurrence:
	•	Token = CandidateLongToken
	•	ContextToken
	•	CoCount

Шаг D — Для каждого (Artikel, ShortToken) выбрать лучший Candidate
	1.	Найди, где в статье встречается ShortToken:

	•	join TokenTable (фильтр Token in ShortTokens) с ArticleContext по Artikelnummer
получишь строки вида:
Artikelnummer, ShortToken, ContextList

	2.	Join на CandidateMap по ShortToken
получишь:
Artikelnummer, ShortToken, CandidateLongToken, ContextList
	3.	Добавь вычисляемую колонку Score:

	•	для каждого ContextToken из ContextList подтянуть CoCount из CandidateCo
	•	суммировать

	4.	Group by (Artikelnummer, ShortToken):

	•	выбрать строку с максимальным Score → это и есть нормализация short→long для этого конкретного артикула.

⸻

6) Итог: нормализация токенов и сборка ClusterKey

Когда у тебя есть таблица замен:
Artikelnummer, ShortToken, NormalizedToken

Ты делаешь:
	•	merge обратно в TokenTable
	•	заменяешь Token на NormalizedToken (если есть)
	•	дальше уже строишь кластерные ключи как раньше.

⸻

Почему это “автоматизированно”, но не “ручной словарь”

Потому что:
	•	кандидаты берутся из данных по префиксам
	•	смысл выбирается по статистике совместных встреч
	•	правила не привязаны к конкретным словам (“kerzen” ты не прописываешь вручную — они всплывают как контекст)

⸻

Практичный компромисс, чтобы стартануть быстрее

Если полный co-occurrence тяжёлый, делай упрощение:
	•	Для каждого ShortToken возьми только 3–5 самых частых CandidateLongToken (по TF)
	•	Score считай не по всему контексту, а по Top 10 контекст-токенам статьи (например, токены с TF в диапазоне 30–500 — “сильные”)

Качество почти не падает, скорость резко лучше.

⸻

Важно: твоя текущая очистка убила числа

Это ок для бренда, но для disambiguation иногда полезно “20x”, “0,5l” и т.п.
Если видишь, что по контексту не хватает сигналов — придётся вернуть хотя бы x/l/ml или сохранить размер отдельной колонкой.

⸻

Если хочешь, я дам конкретный минимальный M-код для “контекстного score и выбора лучшего кандидата”, но мне нужно одно:
как у тебя точно называются запросы/колонки в PQ сейчас (TokenTable, TokenStats, имена столбцов). По скрину вижу TokenTable и TokenStats — если так и есть, можно сразу под них написать.
