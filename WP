Круто, что метрики “зашли” — это реально самый надёжный сигнал в твоих данных. Теперь про автоматизацию “объединить синонимы”.

Здесь ключевое: объединение = найти компоненты связности в графе.
	•	вершины = токены
	•	ребро между A и B, если они “достаточно похожи” по твоим метрикам
	•	дальше каждая компонента = группа синонимов/вариантов
	•	внутри компоненты выбираем канон (обычно самый частотный DF)

Это автоматизируется полностью, без ручных словарей. Ниже — конкретный PQ-пайплайн.

⸻

1) Сначала задаём “правила ребра” (edge filter)

Ты сам решаешь, что считать “синонимом”. Я рекомендую сделать 2 типа связей:

A) “Обрубок → полная форма” (очень надёжно)

Используй:
	•	ContainmentA >= 0.9
	•	DF_B > DF_A
	•	Text.StartsWith(TokenB, TokenA)
	•	(опционально) Text.Length(TokenA) <= 7

Это будет направленное ребро A→B, но для кластеров можно считать как связь.

B) “Две формы одного слова/опечатки” (осторожнее)

Здесь лучше:
	•	Jaccard >= 0.8
	•	min(ContainmentA,ContainmentB) >= 0.7
(это уже “почти одинаковые”)

⸻

2) Построить таблицу “Edges”

Из твоего TokenPairs сделай запрос Edges (оставь только нужные связи):

Колонки:
	•	From = TokenA
	•	To = TokenB

И чтобы граф был неориентированный, добавь “обратные” ребра:
	•	From=TokenB, To=TokenA

(потом просто Append)

⸻

3) Автоматически получить группы (connected components) в Power Query

Power Query не имеет “из коробки” граф-алгоритмов, но connected components можно сделать итеративно (label propagation). Работает отлично на твоих размерах, если токены после фильтра — не десятки тысяч.

Идея label propagation
	1.	каждому токену присваиваем Group = Token (сам себе)
	2.	потом много раз обновляем:
Group(token) = MIN(Group(neighbors))
	3.	когда перестаёт меняться — это компоненты связности

⸻

Готовый M-код: построить Token→Group по Edges

Создай новый query TokenGroups:

let
    // Edges: таблица с колонками From, To (неориентированные ребра)
    Edges0 = Edges,

    // Список всех токенов
    NodesFrom = Table.Distinct(Table.SelectColumns(Edges0, {"From"})),
    NodesTo   = Table.Distinct(Table.RenameColumns(Table.SelectColumns(Edges0, {"To"}), {{"To","From"}})),
    Nodes = Table.Distinct(Table.Combine({NodesFrom, NodesTo})),
    Nodes1 = Table.RenameColumns(Nodes, {{"From","Token"}}),

    // стартовая метка = сам токен
    Init = Table.AddColumn(Nodes1, "Group", each [Token], type text),

    // функция одного шага "распространения метки"
    Step = (state as table) as table =>
        let
            // присоединяем Group соседей
            Join1 = Table.NestedJoin(Edges0, {"From"}, state, {"Token"}, "S", JoinKind.LeftOuter),
            Expand1 = Table.ExpandTableColumn(Join1, "S", {"Group"}, {"FromGroup"}),

            Join2 = Table.NestedJoin(Expand1, {"To"}, state, {"Token"}, "T", JoinKind.LeftOuter),
            Expand2 = Table.ExpandTableColumn(Join2, "T", {"Group"}, {"ToGroup"}),

            // для каждого From берём минимальную группу среди себя и соседей
            Cand = Table.AddColumn(Expand2, "CandGroup",
                    each List.Min(List.RemoveNulls({[FromGroup],[ToGroup]})),
                    type text),

            Agg = Table.Group(Cand, {"From"}, {{"NewGroup", each List.Min([CandGroup]), type text}}),
            Agg2 = Table.RenameColumns(Agg, {{"From","Token"},{"NewGroup","Group"}})
        in
            Agg2,

    // итерации до стабилизации (ограничим 30 шагами)
    IterList = List.Generate(
        () => [i=0, t=Init],
        each [i] < 30,
        each [i=[i]+1, t=Step([t])],
        each [t]
    ),

    // берём последнюю таблицу
    Result = List.Last(IterList)
in
    Result

Получишь:

Token | Group

Где Group — id компоненты (минимальная метка в компоненте).

⸻

4) Выбрать канон внутри группы

Дальше просто:
	1.	Join TokenGroups с TokenDF (DF частота)
	2.	Group by Group, выбрать токен с максимальным DF
	3.	Получится Group → CanonicalToken
	4.	Join обратно: Token → CanonicalToken

Правило канона:
	•	самый частотный DF
	•	при равенстве — самый длинный

⸻

5) Применить к TokenTable

Merge:
TokenTable + Token→CanonicalToken по Token
и заменяешь Token на CanonicalToken.

⸻

6) Что важно, чтобы не множить ошибки

Ты правильно боишься “слишком много допущений”. Чтобы автомат был безопасным:

✅ Делай двухуровневые связи:
	•	“Hard edges” (Containment + StartsWith) — почти безошибочно
	•	“Soft edges” (Jaccard) — только если хочешь расширять

И компоненты строй сначала только по Hard edges.
Потом, если надо, добавляй Soft edges отдельно.

⸻

Практический старт (я бы так сделал)
	1.	Edges = только ContainmentA >= 0.9 + StartsWith
	2.	TokenGroups → Canonical
	3.	применить и посмотреть качество на 200–300 случайных артикулах
	4.	если мало объединений — снижаем containment до 0.85 или добавляем Jaccard 0.85

⸻

Если скажешь:
	•	сколько строк в Edges получается при ContainmentA >= 0.9 + StartsWith
	•	и сколько уникальных токенов там участвует

я подскажу оптимальные пороги, чтобы граф не раздувался и при этом давал нормальную унификацию брендов/обрубков.
