Круто, что метрики “зашли” — это реально самый надёжный сигнал в твоих данных. Теперь про автоматизацию “объединить синонимы”.

Здесь ключевое: объединение = найти компоненты связности в графе.
	•	вершины = токены
	•	ребро между A и B, если они “достаточно похожи” по твоим метрикам
	•	дальше каждая компонента = группа синонимов/вариантов
	•	внутри компоненты выбираем канон (обычно самый частотный DF)

Это автоматизируется полностью, без ручных словарей. Ниже — конкретный PQ-пайплайн.

⸻

1) Сначала задаём “правила ребра” (edge filter)

Ты сам решаешь, что считать “синонимом”. Я рекомендую сделать 2 типа связей:

A) “Обрубок → полная форма” (очень надёжно)

Используй:
	•	ContainmentA >= 0.9
	•	DF_B > DF_A
	•	Text.StartsWith(TokenB, TokenA)
	•	(опционально) Text.Length(TokenA) <= 7

Это будет направленное ребро A→B, но для кластеров можно считать как связь.

B) “Две формы одного слова/опечатки” (осторожнее)

Здесь лучше:
	•	Jaccard >= 0.8
	•	min(ContainmentA,ContainmentB) >= 0.7
(это уже “почти одинаковые”)

⸻

2) Построить таблицу “Edges”

Из твоего TokenPairs сделай запрос Edges (оставь только нужные связи):

Колонки:
	•	From = TokenA
	•	To = TokenB

И чтобы граф был неориентированный, добавь “обратные” ребра:
	•	From=TokenB, To=TokenA

(потом просто Append)

⸻

3) Автоматически получить группы (connected components) в Power Query

Power Query не имеет “из коробки” граф-алгоритмов, но connected components можно сделать итеративно (label propagation). Работает отлично на твоих размерах, если токены после фильтра — не десятки тысяч.

Идея label propagation
	1.	каждому токену присваиваем Group = Token (сам себе)
	2.	потом много раз обновляем:
Group(token) = MIN(Group(neighbors))
	3.	когда перестаёт меняться — это компоненты связности

⸻

Готовый M-код: построить Token→Group по Edges

Создай новый query TokenGroups:

let
    // Edges: таблица с колонками From, To (неориентированные ребра)
    Edges0 = Edges,

    // Список всех токенов
    NodesFrom = Table.Distinct(Table.SelectColumns(Edges0, {"From"})),
    NodesTo   = Table.Distinct(Table.RenameColumns(Table.SelectColumns(Edges0, {"To"}), {{"To","From"}})),
    Nodes = Table.Distinct(Table.Combine({NodesFrom, NodesTo})),
    Nodes1 = Table.RenameColumns(Nodes, {{"From","Token"}}),

    // стартовая метка = сам токен
    Init = Table.AddColumn(Nodes1, "Group", each [Token], type text),

    // функция одного шага "распространения метки"
    Step = (state as table) as table =>
        let
            // присоединяем Group соседей
            Join1 = Table.NestedJoin(Edges0, {"From"}, state, {"Token"}, "S", JoinKind.LeftOuter),
            Expand1 = Table.ExpandTableColumn(Join1, "S", {"Group"}, {"FromGroup"}),

            Join2 = Table.NestedJoin(Expand1, {"To"}, state, {"Token"}, "T", JoinKind.LeftOuter),
            Expand2 = Table.ExpandTableColumn(Join2, "T", {"Group"}, {"ToGroup"}),

            // для каждого From берём минимальную группу среди себя и соседей
            Cand = Table.AddColumn(Expand2, "CandGroup",
                    each List.Min(List.RemoveNulls({[FromGroup],[ToGroup]})),
                    type text),

            Agg = Table.Group(Cand, {"From"}, {{"NewGroup", each List.Min([CandGroup]), type text}}),
            Agg2 = Table.RenameColumns(Agg, {{"From","Token"},{"NewGroup","Group"}})
        in
            Agg2,

    // итерации до стабилизации (ограничим 30 шагами)
    IterList = List.Generate(
        () => [i=0, t=Init],
        each [i] < 30,
        each [i=[i]+1, t=Step([t])],
        each [t]
    ),

    // берём последнюю таблицу
    Result = List.Last(IterList)
in
    Result

Получишь:

Token | Group

Где Group — id компоненты (минимальная метка в компоненте).

⸻

4) Выбрать канон внутри группы

Дальше просто:
	1.	Join TokenGroups с TokenDF (DF частота)
	2.	Group by Group, выбрать токен с максимальным DF
	3.	Получится Group → CanonicalToken
	4.	Join обратно: Token → CanonicalToken

Правило канона:
	•	самый частотный DF
	•	при равенстве — самый длинный

⸻

5) Применить к TokenTable

Merge:
TokenTable + Token→CanonicalToken по Token
и заменяешь Token на CanonicalToken.

⸻

6) Что важно, чтобы не множить ошибки

Ты правильно боишься “слишком много допущений”. Чтобы автомат был безопасным:

✅ Делай двухуровневые связи:
	•	“Hard edges” (Containment + StartsWith) — почти безошибочно
	•	“Soft edges” (Jaccard) — только если хочешь расширять

И компоненты строй сначала только по Hard edges.
Потом, если надо, добавляй Soft edges отдельно.

⸻

Практический старт (я бы так сделал)
	1.	Edges = только ContainmentA >= 0.9 + StartsWith
	2.	TokenGroups → Canonical
	3.	применить и посмотреть качество на 200–300 случайных артикулах
	4.	если мало объединений — снижаем containment до 0.85 или добавляем Jaccard 0.85

⸻

Если скажешь:
	•	сколько строк в Edges получается при ContainmentA >= 0.9 + StartsWith
	•	и сколько уникальных токенов там участвует

я подскажу оптимальные пороги, чтобы граф не раздувался и при этом давал нормальную унификацию брендов/обрубков.

Ты прав — в том коде я специально убрал “вторую ориентацию пары” строкой:

OneSide = Table.SelectRows(NoSelf, each [TokenA] < [TokenB])

Это делалось, чтобы не удваивать пары (A,B) и (B,A) при расчёте Jaccard.
Но для варианта А (обрубок → полная форма) ориентация важна, и такой шаг действительно может “спрятать” нужные связи.

Что именно “ломается”

Для правила Text.StartsWith(Long, Short) тебе нужно, чтобы в строке явно было:
	•	Short = adventsk
	•	Long = adventskalender

А фильтр TokenA < TokenB оставляет ориентацию по алфавиту, а не по длине/вложенности. Поэтому часть “правильных” направленных пар может оказаться выкинутой.

⸻

✅ Исправление: не алфавит, а Short/Long по длине

Нужно заменить логику “оставить одну сторону” на логику:
	•	ShortToken = более короткий
	•	LongToken = более длинный
(а если равны — можно игнорировать, для варианта А это не важно)

И уже потом считать overlap по (ShortToken, LongToken).

⸻

Готовый M-код (замена блока в TokenPairs)

Ниже кусок, который заменяет шаги NoSelf + OneSide и делает правильные directed-пары для варианта А.

Вставь после того, как у тебя есть таблица с колонками:
Artikelnummer, TokenA, TokenB (после self-join expand/rename).

// 4) Убираем A=A
NoSelf = Table.SelectRows(Renamed, each [TokenA] <> [TokenB]),

// 5) Строим ориентированную пару Short/Long по длине
AddShort = Table.AddColumn(NoSelf, "ShortToken",
    each if Text.Length([TokenA]) <= Text.Length([TokenB]) then [TokenA] else [TokenB],
    type text),

AddLong = Table.AddColumn(AddShort, "LongToken",
    each if Text.Length([TokenA]) <= Text.Length([TokenB]) then [TokenB] else [TokenA],
    type text),

// 6) Убираем равную длину (для варианта А не нужно)
OnlyStrict = Table.SelectRows(AddLong, each Text.Length([ShortToken]) < Text.Length([LongToken])),

// 7) Убираем дубли внутри артикула (на всякий случай)
DedupPairsInArtikel = Table.Distinct(OnlyStrict, {"Artikelnummer","ShortToken","LongToken"}),

// 8) Считаем Overlap по (ShortToken, LongToken)
PairCounts = Table.Group(DedupPairsInArtikel, {"ShortToken","LongToken"},
    {{"Overlap", each Table.RowCount(_), Int64.Type}})

Дальше вместо TokenA/TokenB в join’ах с DF используй ShortToken/LongToken.

⸻

Дальше — фильтр “вариант А”

После того как ты подтянул DF_Short, DF_Long, добавь условия:
	•	Text.StartsWith(LongToken, ShortToken)
	•	ContainmentShort = Overlap / DF_Short >= 0.9
	•	DF_Long > DF_Short (опционально)

Пример добавления колонки containment и фильтра:

AddContainShort = Table.AddColumn(PrevStep, "ContainmentShort",
    each Number.From([Overlap]) / Number.From([DF_Short]), type number),

HardEdges = Table.SelectRows(AddContainShort,
    each [ContainmentShort] >= 0.9 and Text.StartsWith([LongToken], [ShortToken]))


⸻

Почему это решает твою претензию

Потому что мы больше не теряем пары из-за лексикографического TokenA < TokenB.
Мы всегда приводим пару к виду (короткий, длинный) и уже потом применяем StartsWith/Containment.

⸻

Если хочешь, я могу прямо собрать тебе полный TokenPairs_A (обрубок→полная) запрос целиком, чтобы ты вставил одним куском без правок — скажи только точные имена колонок в исходной таблице (у тебя, кажется, Artikelnummer и Token).
